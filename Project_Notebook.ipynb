{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chinmayeeadiga/emotion-detection/blob/main/Project_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viZzaONjavgK"
      },
      "source": [
        "**EMOTION DETECTION BY TEXT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6t28ZR0aLo6"
      },
      "source": [
        "Number of emotion classes : 6\n",
        "\n",
        "Types of emotion classes :\n",
        "*   Sadness\n",
        "*   Joy\n",
        "*   Love\n",
        "*   Anger\n",
        "*   Fear\n",
        "*   Surprise\n",
        "\n",
        "Dataset Used : Emotion Dataset by dair-ai\n",
        "\n",
        "Models Used : Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "928GybnWDiN5"
      },
      "outputs": [],
      "source": [
        "#importing dependencies\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b3npnS9DiKZ"
      },
      "outputs": [],
      "source": [
        "#loading the dataset\n",
        "splits = {'train': 'split/train-00000-of-00001.parquet', 'validation': 'split/validation-00000-of-00001.parquet', 'test': 'split/test-00000-of-00001.parquet'}\n",
        "X_train = pd.read_parquet(\"hf://datasets/dair-ai/emotion/\" + splits[\"train\"])\n",
        "X_val = pd.read_parquet(\"hf://datasets/dair-ai/emotion/\" + splits[\"validation\"])\n",
        "X_test = pd.read_parquet(\"hf://datasets/dair-ai/emotion/\" + splits[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oIPVK7QDiHf"
      },
      "outputs": [],
      "source": [
        "# cleaning the data\n",
        "!pip install wordcloud emoji\n",
        "import re\n",
        "import emoji\n",
        "import string\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|@\\w+\", '', text)\n",
        "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens\n",
        "              if word not in stop_words and word.isalpha()]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "X_train['clean_text'] = X_train['text'].apply(preprocess)\n",
        "X_test['clean_text'] = X_test['text'].apply(preprocess)\n",
        "X_val['clean_text'] = X_val['text'].apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYVHBrubDiEw"
      },
      "outputs": [],
      "source": [
        "# getting the data ready\n",
        "y_train = X_train['label']\n",
        "y_test = X_test['label']\n",
        "y_val = X_val['label']\n",
        "\n",
        "X_train = X_train[['clean_text']]\n",
        "X_train.columns = ['clean_text']\n",
        "X_test = X_test[['clean_text']]\n",
        "X_test.columns = ['clean_text']\n",
        "X_val = X_val[['clean_text']]\n",
        "X_val.columns = ['clean_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VISUALIZATION**"
      ],
      "metadata": {
        "id": "RJDp-h3XazFd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4onOjyg8b1uA"
      },
      "outputs": [],
      "source": [
        "#bar plots\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x=y_train, palette='Set2')\n",
        "plt.title(\"Label Distribution in y_train\")\n",
        "plt.xlabel(\"Emotion Label\")\n",
        "plt.ylabel(\"Number of Samples\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_bQ2ns6cKCJ"
      },
      "outputs": [],
      "source": [
        "#word clouds\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "text = \" \".join(X_train['clean_text'].tolist())\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Word Cloud of X_train\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf_GfBh1EkUD"
      },
      "source": [
        "**WORD EMBEDDINGS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24lD2yWEDh_i"
      },
      "outputs": [],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOeFFyWzDh8u"
      },
      "outputs": [],
      "source": [
        "embeddings_index = {}\n",
        "with open(\"glove.6B.100d.txt\", encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LB1pFOz8Dh5h"
      },
      "outputs": [],
      "source": [
        "#Tokenization\n",
        "tokenizer = Tokenizer(num_words=20000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X_train['clean_text'])\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train['clean_text'])\n",
        "X_val_seq = tokenizer.texts_to_sequences(X_val['clean_text'])\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test['clean_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr_iIgHeIfWc"
      },
      "outputs": [],
      "source": [
        "#Padding\n",
        "max_len = max(len(seq) for seq in X_train_seq)\n",
        "X_train = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
        "X_val = pad_sequences(X_val_seq, maxlen=max_len, padding='post')\n",
        "X_test = pad_sequences(X_test_seq, maxlen=max_len, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xb_Q-lbDhuh"
      },
      "outputs": [],
      "source": [
        "#Load GloVe Embeddings\n",
        "embedding_index = {}\n",
        "with open('/content/glove.6B.100d.txt', encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qh955VTyIj0o"
      },
      "outputs": [],
      "source": [
        "#Create Embedding Matrix\n",
        "embedding_dim = 100\n",
        "word_index = tokenizer.word_index\n",
        "num_words = min(20000, len(word_index) + 1)\n",
        "\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i >= num_words:\n",
        "        continue\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRutUJ2zE50L"
      },
      "source": [
        "**CNN MODELS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUcl4bMjZ5AJ"
      },
      "outputs": [],
      "source": [
        "#Prepare labels (convert to categorical)\n",
        "num_classes = y_train.nunique()\n",
        "y_train_new = to_categorical(y_train, num_classes=num_classes)\n",
        "y_val_new = to_categorical(y_val, num_classes=num_classes)\n",
        "y_test_new = to_categorical(y_test, num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzllHWmPD7rw"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Concatenate\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "input_layer = Input(shape=(max_len,))\n",
        "\n",
        "embedding_layer = Embedding(input_dim=num_words,\n",
        "                            output_dim=embedding_dim,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=max_len,\n",
        "                            trainable=True)(input_layer)\n",
        "\n",
        "conv_3 = Conv1D(128, kernel_size=3, activation='relu')(embedding_layer)\n",
        "conv_3 = BatchNormalization()(conv_3)\n",
        "pool_3 = GlobalMaxPooling1D()(conv_3)\n",
        "\n",
        "conv_4 = Conv1D(128, kernel_size=4, activation='relu')(embedding_layer)\n",
        "conv_4 = BatchNormalization()(conv_4)\n",
        "pool_4 = GlobalMaxPooling1D()(conv_4)\n",
        "\n",
        "conv_5 = Conv1D(128, kernel_size=5, activation='relu')(embedding_layer)\n",
        "conv_5 = BatchNormalization()(conv_5)\n",
        "pool_5 = GlobalMaxPooling1D()(conv_5)\n",
        "\n",
        "merged = Concatenate()([pool_3, pool_4, pool_5])\n",
        "\n",
        "dropout = Dropout(0.5)(merged)\n",
        "dense = Dense(128, activation='relu')(dropout)\n",
        "output = Dense(num_classes, activation='softmax')(dense)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8v-kQa9FED1Y"
      },
      "outputs": [],
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('best_cnn_model.h5', save_best_only=True, monitor='val_loss')\n",
        "\n",
        "history = model.fit(X_train, y_train_new,\n",
        "                    epochs=20,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(X_val, y_val_new),\n",
        "                    callbacks=[early_stop, checkpoint])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtlWvsFpUZEx"
      },
      "outputs": [],
      "source": [
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "y_true = np.argmax(y_test_new, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiqIzDncEHAI"
      },
      "outputs": [],
      "source": [
        "# evaluating the model\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**USING THE MODEL**"
      ],
      "metadata": {
        "id": "HR_C-jKva76O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "max_len = 35\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = emoji.demojize(text)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "import gradio as gr\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y_train)\n",
        "\n",
        "def predict_emotion(text):\n",
        "    cleaned = preprocess_text(text)\n",
        "    seq = tokenizer.texts_to_sequences([cleaned])\n",
        "    padded = pad_sequences(seq, maxlen=max_len, padding=\"post\")\n",
        "    pred = model.predict(padded)[0]\n",
        "    threshold = 0.3\n",
        "\n",
        "    top_indices = np.where(pred > threshold)[0]\n",
        "    if len(top_indices) == 0:\n",
        "        return \"Neutral\"\n",
        "\n",
        "    emotions = [str(e) for e in label_encoder.classes_[top_indices]]\n",
        "    return \", \".join(emotions)\n"
      ],
      "metadata": {
        "id": "jhS1aS-Cc95j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "input_box = widgets.Textarea(placeholder='Enter text here', layout=widgets.Layout(width='100%', height='100px'))\n",
        "output_box = widgets.Output()\n",
        "button = widgets.Button(description='Detect Emotion')\n",
        "\n",
        "def on_click(b):\n",
        "    output_box.clear_output()\n",
        "    text = input_box.value\n",
        "    with output_box:\n",
        "        print(predict_emotion(text))\n",
        "\n",
        "button.on_click(on_click)\n",
        "\n",
        "display(input_box, button, output_box)\n"
      ],
      "metadata": {
        "id": "u7qeNO2ikSdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "files.download(\"best_cnn_model.h5\")\n",
        "\n",
        "with open('tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "files.download(\"tokenizer.pkl\")\n",
        "\n",
        "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "files.download(\"label_encoder.pkl\")"
      ],
      "metadata": {
        "id": "GuT7EJ7ugpbu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/rvmL+nBZ8XfJ6DEVOxQf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}